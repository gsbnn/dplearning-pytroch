{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 线性回归的简洁实现  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 加载数据集  \n",
    "注意数据集的维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 2]) torch.Size([1000, 1])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils import data\n",
    "from d2l import torch as d2l\n",
    "from torch import nn\n",
    "\n",
    "true_w = torch.tensor([2, -3.4])\n",
    "true_b = torch.tensor([4.2])\n",
    "features, labels = d2l.synthetic_data(true_w, true_b, 1000)\n",
    "print(features.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 构造小批量样本生产函数  \n",
    "`torch.utils.data.DataLoader()`函数详解：这里主要介绍自动批次加载数据，关注三个点：输出顺序，一个批次的大小，以何种形式输出批次。  \n",
    ">```Python\n",
    ">torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=None, sampler=None, batch_sampler=None, num_workers=0,  \n",
    ">                            collate_fn=None, pin_memory=False, drop_last=False, timeout=0, worker_init_fn=None,  \n",
    ">                            multiprocessing_context=None, generator=None, *, prefetch_factor=None, persistent_workers=False,  \n",
    ">                            pin_memory_device='', in_order=True)  \n",
    ">Parameters  \n",
    ">   \n",
    ">   *  dataset(Dataset) - Map-style datasets or Iterable-style datasets.A map-style dataset is one that implements the __getitem__() and __len__() protocols, and represents a map from (possibly non-integral) indices/keys to data samples. An iterable-style dataset is an instance of a subclass of IterableDataset that implements the __iter__() protocol, and represents an iterable over data samples.  \n",
    ">   *  batch_size(int, optional) - 一个batch的样本数\n",
    ">   *  shuffle(bool, optional) - 随机打乱datasets的样本输出顺序\n",
    ">   *  sampler(Sampler or Iterable, optional) - 定义dataset的样本输出顺序，每次输出一个样本，不能与shuffle同时使用（For map-style datasets, the sampler is either provided by user or constructed based on the shuffle argument.）  \n",
    ">   *  batch_sampler(Sampler or Iterable, optional)  - 定义dataset的样本输出顺序和batch_size，不能与batch_size、shuffle、sampler同时使用\n",
    ">   *  collate_fn(Callable, optional)  - 一个定义如何打包输出样本的函数，有默认值  \n",
    ">```  \n",
    "\n",
    "`torch.utils.data.TensorDateset()`类详解：  \n",
    ">```python\n",
    "> def __init__(self, *tensors: Tensor) -> None: 输入是多个最高维度相同的tensor，例如tensor1, tensor2, ..., tensorx  \n",
    "> def __getitem__(self, index):  输出是tuple(tensor1[index], tensor2[index], ..., tensorx[index])  \n",
    ">```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[-0.0864, -0.3470],\n",
      "        [-0.4102, -0.4229],\n",
      "        [ 1.1013, -0.7217],\n",
      "        [-0.3959, -0.8336],\n",
      "        [ 0.0923, -0.3248],\n",
      "        [-1.4870, -0.8908],\n",
      "        [ 0.4897,  0.0634],\n",
      "        [-0.5066, -0.3753],\n",
      "        [-0.2499, -0.9528],\n",
      "        [ 0.0209, -0.6818]]), tensor([[5.2185],\n",
      "        [4.8035],\n",
      "        [8.8540],\n",
      "        [6.2350],\n",
      "        [5.4861],\n",
      "        [4.2617],\n",
      "        [4.9746],\n",
      "        [4.4595],\n",
      "        [6.9510],\n",
      "        [6.5486]])]\n"
     ]
    }
   ],
   "source": [
    "def load_array(data_arrays, batch_size, is_train=True):\n",
    "    \"\"\"构造一个PyTorch数据迭代器\"\"\"\n",
    "    dataset = data.TensorDataset(*data_arrays)\n",
    "    return data.DataLoader(dataset, batch_size, shuffle=is_train)\n",
    "\n",
    "batch_size = 10\n",
    "data_iter = load_array((features, labels), batch_size)\n",
    "print(next(iter(data_iter)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 定义模型  \n",
    "`nn.Linear()`函数详解：  \n",
    ">```Python\n",
    ">torch.nn.Linear(in_features, out_features, bias=True, device=None, dtype=None)\n",
    "> Parameters  \n",
    ">   *  in_features (int) – size of each input sample  \n",
    ">   *  out_features (int) – size of each output sample  \n",
    ">   *  bias(bool)** – If set to False, the layer will not learn an additive bias. Default: True  \n",
    "> # 举例  \n",
    "> m = nn.Linear(20, 30) # 输入样本的features数为20，输出样本的features数为30  \n",
    "> input = torch.randn(128, 20) # 输入128个样本，每个样本的features数为20   \n",
    "> output = m(input)  \n",
    "> print(output.size())\n",
    "> torch.Size([128, 30]) # 输出128个样本，每个样本的features数为30  \n",
    ">```  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential(nn.Linear(2, 1))\n",
    "net[0].weight.data.normal_(0, 0.01)\n",
    "net[0].bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 定义损失函数  \n",
    "`nn.MSELoss()`计算的是误差平均值。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.MSELoss() # 生成一个MSELoss实例，包含该实例具有方法__call__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 定义优化算法  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = torch.optim.SGD(net.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch1, loss0.000106\n",
      "epoch2, loss0.000106\n",
      "epoch3, loss0.000106\n",
      "epoch4, loss0.000106\n",
      "epoch5, loss0.000106\n",
      "epoch6, loss0.000106\n",
      "epoch7, loss0.000106\n",
      "epoch8, loss0.000106\n",
      "epoch9, loss0.000106\n",
      "torch.FloatTensor\n",
      "w的估计误差： tensor([2.8467e-04, 1.5736e-05])\n",
      "b的估计误差： tensor([-0.0005])\n",
      "w的梯度： None\n",
      "b的梯度： None\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 9\n",
    "for epoch in range(num_epochs):\n",
    "    for X, y in data_iter:\n",
    "        l = loss(net(X), y) # 带入模型，计算损失\n",
    "        trainer.zero_grad() # 将参数之前的梯度清除\n",
    "        l.backward() # 反向传播，获取梯度\n",
    "        trainer.step() # 优化，参数更新，SGD用@torch.no_grad()修饰了\n",
    "        \n",
    "    l = loss(net(features), labels) # 注意这里的l与上面循环中的l不是同一个，因此不用torch.no_grad()也可以\n",
    "    print(f'epoch{epoch + 1}, loss{l:f}')\n",
    "\n",
    "l.backward()\n",
    "w = net[0].weight.data\n",
    "b = net[0].bias.data\n",
    "print('w的估计误差：', true_w - w.reshape(true_w.shape))\n",
    "print('b的估计误差：', true_b - b)\n",
    "print('w的梯度：', net[0].weight.data.grad)\n",
    "print('b的梯度：', net[0].bias.data.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2850161550160\n",
      "2850182067648\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(10, dtype=torch.float32)\n",
    "print(id(a))\n",
    "a = torch.arange(10, dtype=torch.float32)\n",
    "print(id(a))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
