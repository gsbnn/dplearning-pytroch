{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pytorch中的线性代数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 张量的Hadamard积  \n",
    "在python中执行`A * B`即可。  \n",
    "\n",
    "![张量的Hadamard积](picture\\Hadamard_product.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  0.,   1.,   4.,   9.],\n",
      "        [ 16.,  25.,  36.,  49.],\n",
      "        [ 64.,  81., 100., 121.],\n",
      "        [144., 169., 196., 225.],\n",
      "        [256., 289., 324., 361.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "A = torch.arange(20, dtype=torch.float32).reshape(5, 4)\n",
    "B = A.clone()\n",
    "B[:] = A * B\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 降维求和  \n",
    "理解torch.sum()中的`dim`参数，dim表示要减少的维度  \n",
    "非降维求和需要torch.sum()中的`keepdim = True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.],\n",
      "        [12., 13., 14., 15.],\n",
      "        [16., 17., 18., 19.]])\n",
      "tensor([40., 45., 50., 55.]) torch.Size([4])\n",
      "tensor([ 6., 22., 38., 54., 70.]) torch.Size([5])\n",
      "tensor(190.)\n",
      "tensor([[[ 0.,  1.],\n",
      "         [ 2.,  3.],\n",
      "         [ 4.,  5.],\n",
      "         [ 6.,  7.],\n",
      "         [ 8.,  9.]],\n",
      "\n",
      "        [[10., 11.],\n",
      "         [12., 13.],\n",
      "         [14., 15.],\n",
      "         [16., 17.],\n",
      "         [18., 19.]]])\n",
      "tensor([ 45., 145.])\n",
      "tensor([ 90., 100.])\n",
      "tensor([22., 30., 38., 46., 54.])\n"
     ]
    }
   ],
   "source": [
    "print(A)\n",
    "A_sum_axis0 = A.sum(dim=0) # 行降维，按列求和\n",
    "print(A_sum_axis0, A_sum_axis0.shape) \n",
    "\n",
    "A_sum_axis1 = A.sum(dim=1) # 列降维，按行求和\n",
    "print(A_sum_axis1, A_sum_axis1.shape)\n",
    "\n",
    "A_sum = A.sum() # 整体求和\n",
    "print(A_sum)\n",
    "\n",
    "C = torch.arange(20, dtype=torch.float32).reshape(2, 5, 2)\n",
    "print(C)\n",
    "print(C.sum(dim=(1, 2))) # 依次减少1,2维\n",
    "print(C.sum(dim=(0, 1)))\n",
    "print(C.sum(dim=(0, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 求平均值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 8.,  9., 10., 11.])\n",
      "tensor([ 8.,  9., 10., 11.])\n",
      "tensor([ 1.5000,  5.5000,  9.5000, 13.5000, 17.5000])\n",
      "tensor([ 1.5000,  5.5000,  9.5000, 13.5000, 17.5000])\n",
      "tensor(9.5000)\n",
      "tensor(9.5000)\n"
     ]
    }
   ],
   "source": [
    "A_mean_axis0 = A.mean(dim=0) # 行降维，按列求和\n",
    "print(A_mean_axis0)\n",
    "A_mean_axis0 = A.sum(dim=0) / A.shape[0]\n",
    "print(A_mean_axis0)\n",
    "\n",
    "A_mean_axis1 = A.mean(dim=1) # 列降维，按行求和\n",
    "print(A_mean_axis1)\n",
    "A_mean_axis1 = A.sum(dim=1) / A.shape[1]\n",
    "print(A_mean_axis1)\n",
    "\n",
    "A_mean = A.mean()\n",
    "print(A_mean)\n",
    "A_mean = A.sum() / A.numel()\n",
    "print(A_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 矩阵乘法\n",
    "> - 向量乘：`torch.dot(x, y)`或`x@y`  \n",
    "> - 向量与矩阵相乘：`torch.mv(A, x)`或`A@x`  \n",
    "> - 矩阵与矩阵相乘：`torch.mm(A, B)`或`A@B`  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 范数  \n",
    "- 范数：向量的大小，满足以下性质：  \n",
    "\n",
    "![性质1](picture\\norms1.jpg)  \n",
    "![性质2和3](picture\\norms2.jpg)  \n",
    "\n",
    "- L2范数：欧几里得距离就属于L2范数。  \n",
    "\n",
    "![L2范数](picture\\norms3.jpg)\n",
    "\n",
    "- L1范数：向量元素绝对值之和。与L2范数相比， L1范数受异常值的影响较小。  \n",
    "\n",
    "![L1范数](picture\\norms4.jpg)  \n",
    "\n",
    "- L2范数和L1范数都是更一般的Lp范数的特例：  \n",
    "\n",
    "![Lp范数](picture\\norms5.jpg)  \n",
    "\n",
    "- 类似于向量的L2范数，矩阵X ∈ Rm×n的Frobenius范数（Frobenius norm）是矩阵元素平方和的平方根：  \n",
    "\n",
    "![Frobenius范数](picture\\norms6.jpg)  \n",
    "Frobenius范数满足向量范数的所有性质，它就像是矩阵形向量的L2范数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.)\n",
      "tensor(5.4772)\n",
      "tensor([6., 6.])\n"
     ]
    }
   ],
   "source": [
    "u = torch.arange(5, dtype=torch.float32)\n",
    "\n",
    "# L1范数\n",
    "print(torch.linalg.vector_norm(u, ord=1))\n",
    "\n",
    "# L2范数\n",
    "print(torch.linalg.vector_norm(u, ord=2))\n",
    "\n",
    "# Frobenius范数，dim=(-2, -1)为默认值，将最低的两个维度进行范数计算\n",
    "# 更高维度的值作为批次数。\n",
    "print(torch.linalg.matrix_norm(torch.ones(2, 4, 9), ord='fro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 练习  \n",
    "- 本节中定义了形状(2, 3, 4)的张量X。 len(X)的输出结果是什么？  \n",
    "答：最高维度的值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.,  1.,  2.,  3.],\n",
      "         [ 4.,  5.,  6.,  7.],\n",
      "         [ 8.,  9., 10., 11.]],\n",
      "\n",
      "        [[12., 13., 14., 15.],\n",
      "         [16., 17., 18., 19.],\n",
      "         [20., 21., 22., 23.]]])\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(24, dtype=torch.float32).reshape(2, 3, 4)\n",
    "print(x)\n",
    "print(len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 运行A/A.sum(dim=1)，看看会发生什么。请分析一下原因？  \n",
    "A.sum(dim=1)发生数据降维，且不满足广播机制条件，应采用A.sum(dim=1， keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  1.,  2.,  3.,  4.],\n",
      "        [ 5.,  6.,  7.,  8.,  9.],\n",
      "        [10., 11., 12., 13., 14.],\n",
      "        [15., 16., 17., 18., 19.]])\n",
      "tensor([[10.],\n",
      "        [35.],\n",
      "        [60.],\n",
      "        [85.]]) torch.Size([4, 1])\n",
      "tensor([[0.0000, 0.1000, 0.2000, 0.3000, 0.4000],\n",
      "        [0.1429, 0.1714, 0.2000, 0.2286, 0.2571],\n",
      "        [0.1667, 0.1833, 0.2000, 0.2167, 0.2333],\n",
      "        [0.1765, 0.1882, 0.2000, 0.2118, 0.2235]])\n"
     ]
    }
   ],
   "source": [
    "X = torch.arange(20, dtype=torch.float32).reshape(4, 5)\n",
    "print(X)\n",
    "X_sum_axis1 = X.sum(dim=1, keepdim=True)\n",
    "#X_sum_axis1 = X.sum(dim=1)\n",
    "print(X_sum_axis1, X_sum_axis1.shape)\n",
    "print(X/X_sum_axis1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pytorch中的张量乘法`torch.matmul()`详解  \n",
    "    1. 如果是两个一维张量相乘，维度均为(N)，则进行向量点积，结果为标量，即$z=\\bold{xy}$。  \n",
    "    2. 如果是二维张量(M, N)*一维张量(N)，则将一维张量**右侧升维**，即(N, 1)，然后进行矩阵运算得到矩阵维度为(M, 1)，再降维得到最终结果的维度为(M)。  \n",
    "    3. 如果是一维张量(N)*二维张量(N, M)，则将一维张量**左侧升维**，即(1, N)，然后进行矩阵运算得到矩阵维度为(1, M)，再降维得到最终结果的维度为(M)。   \n",
    "    4. 如果2,3中所描述的二维张量扩展到更高维度（如三维张量），一维张量升维准则不变，对于超过二维的维度进行广播，最终计算结果不进行降维。具体维度变化如下：  \n",
    "    >以(K, M, N)*(N)为例  \n",
    "    >$$\n",
    "    >\\bold{(K, M, N)}\\times\\bold{(N)} \\\\\n",
    "    >\\bold{(N)}\\rightarrow\\bold{(K, N, 1)} \\\\\n",
    "    >\\bold{(K, M, N)}\\times\\bold{(N)}\\rightarrow\\bold{(K, M, N)}\\times\\bold{(K, N, 1)}=\\bold{(K, M, 1)}\n",
    "    >$$  \n",
    "\n",
    "    >以(N)*(K, N, M)为例  \n",
    "    >$$\n",
    "    >\\bold{(N)}\\times\\bold{(K, N, M)} \\\\\n",
    "    >\\bold{(N)}\\rightarrow\\bold{(K, 1, N)} \\\\\n",
    "    >\\bold{(N)}\\times\\bold{(K, N, M)}\\rightarrow\\bold{(K, 1, N)}\\times\\bold{(K, N, M)}=\\bold{(K, 1, M)}\n",
    "    >$$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 2.])\n",
      "tensor([[[ 0.,  1.,  2.],\n",
      "         [ 3.,  4.,  5.]],\n",
      "\n",
      "        [[ 6.,  7.,  8.],\n",
      "         [ 9., 10., 11.]]])\n",
      "tensor([[ 5., 14.],\n",
      "        [23., 32.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(3, dtype=torch.float32)\n",
    "b = torch.arange(12, dtype=torch.float32).reshape(2, 2, 3)\n",
    "c = torch.matmul(b, a)\n",
    "# d = torch.matmul(b, a)\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)\n",
    "# print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.9801, -0.9638, -0.1028,  0.0226],\n",
      "        [ 0.1911,  1.6425, -1.1452, -0.8970],\n",
      "        [ 1.2729, -1.8892, -0.1872, -1.4800]])\n",
      "tensor([-0.5190,  0.6274, -0.1808, -0.0306])\n",
      "tensor([-1.0954,  1.1657, -1.7667])\n",
      "tensor([[0., 1., 2., 3.]])\n",
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [3.]])\n"
     ]
    }
   ],
   "source": [
    "tensor1 = torch.randn(3, 4)\n",
    "tensor2 = torch.randn(4)\n",
    "tensor3 = torch.matmul(tensor1, tensor2)\n",
    "tensor4 = torch.arange(4, dtype=torch.float32).reshape(1, 4)\n",
    "tensor5 = torch.arange(4, dtype=torch.float32).reshape(4, 1)\n",
    "print(tensor1)\n",
    "print(tensor2)\n",
    "print(tensor3)\n",
    "print(tensor4)\n",
    "print(tensor5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
