{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# softmax回归的从零开始实现  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils import data\n",
    "from torchvision import transforms\n",
    "from IPython import display\n",
    "from d2l import torch as d2l\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "def get_dataloader_workers(): \n",
    "    \"\"\"使用4个进程来读取数据\"\"\"\n",
    "    return 4\n",
    "  \n",
    "def load_data_fashion_mnist(batch_size, resize=None):\n",
    "    \"\"\"下载Fashion-MNIST数据集，然后将其加载到内存中\"\"\"\n",
    "    # torchvision.transform模块功能如下：\n",
    "    # 1.支持PIL Image or ndarray和tensor之间相互转换\n",
    "    # 2.支持tensor的dtype转换\n",
    "\n",
    "    # torchvision.transforms.ToTensor()转换结果如下：\n",
    "    # 1.图片数据（data）：tensor，torch.size([C,H,W])，C为通道数\n",
    "    # 2.图片标签（labels）：int类型\n",
    "    trans = [transforms.ToTensor()] \n",
    "    if resize:\n",
    "        trans.insert(0, transforms.Resize(resize))\n",
    "    trans = transforms.Compose(trans)\n",
    "\n",
    "    \n",
    "    # 根据定义的转换规则trans，将数据转换到mnist_train和mnist_test\n",
    "    # 具体过程为，索引数据时，__getitem__()输出为：tuple(tensor(C, H, W), int)\n",
    "    mnist_train = torchvision.datasets.FashionMNIST( \n",
    "        root=\"data\", train=True, transform=trans, download=True\n",
    "    )\n",
    "    mnist_test = torchvision.datasets.FashionMNIST(\n",
    "        root=\"data\", train=False, transform=trans, download=True\n",
    "    )\n",
    "\n",
    "    # data.DataLoader()接收tuple类型\n",
    "    # 并将tuple中每个元素按batch_size打包成一个tensor\n",
    "    # 最后将每个元素的tensor装进一个列表\n",
    "    # 数据类型变化如下：\n",
    "    # tuple(tensor, int)--->list[tensor, tensor]\n",
    "    # 数据维度变化如下：\n",
    "    # tuple(data(C, H, W), labels(None))--->list[data(batch_size, C, H, W), labels(batch_size)]\n",
    "    return (data.DataLoader(mnist_train, batch_size, shuffle=True, \n",
    "                            num_workers=get_dataloader_workers()), \n",
    "            data.DataLoader(mnist_test, batch_size, shuffle=False, \n",
    "                            num_workers=get_dataloader_workers()))\n",
    "\n",
    "train_iter, test_iter = load_data_fashion_mnist(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 初始化模型参数  \n",
    "每个样本都将用固定长度的向量表示。原始数据集中的每个样本都是28×28的图像。本节将展平每个图像，把它们看作长度为784的向量。在后面的章节中，我们将讨论能够利用图像空间结构的特征，但现在我们暂时只把每个像素位置看作一个特征。  \n",
    "\n",
    "在softmax回归中，我们的输出与类别一样多。因为我们的数据集有10个类别，所以网络输出维度为10。因此，权重将构成一个784 × 10的矩阵，偏置将构成一个1 × 10的行向量。与线性回归一样，我们将使用正态分布初始化我们的权重W，偏置初始化为0。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs = 784\n",
    "num_outputs = 10\n",
    "W = torch.normal(0, 0.01, size=(num_inputs, num_outputs), requires_grad=True) # 注意开启计算图记录\n",
    "b = torch.zeros(num_outputs, requires_grad=True) # b是一个一维Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 定义softmax操作  \n",
    "网络的输出矩阵$\\bold O$的每一行是数据集中每一行（即一个样本）的变化结果。  \n",
    "实现softmax由三个步骤组成：  \n",
    "\n",
    "1.对每个项求幂（使用exp）；  \n",
    "2.对每一行求和（小批量中每个样本是一行），得到每个样本的规范化常数；  \n",
    "3.将每一行除以其规范化常数，确保结果的和为1。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    # (batch_size, q)--->(batch_size, q)\n",
    "    X_exp = torch.exp(X) # (batch_size, q)\n",
    "    partition = X_exp.sum(dim=1, keepdim=True) # (batch_size, 1)\n",
    "    return X_exp / partition # 运用广播机制\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意，虽然这在数学上看起来是正确的，但我们在代码实现中有点草率。**矩阵中的非常大或非常小的元素可能造成数值上溢或下溢**，但我们没有采取措施来防止这点。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 定义模型  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net(X):\n",
    "    # (batch_size, C, H, w)--->(batch_size, q)\n",
    "    return softmax(torch.matmul(X.reshape(-1, W.shape[0]), W) + b) # (batch_size, q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 定义损失函数  \n",
    "y_hat即输出概率，维度为(batch_size, q)，y为样本实际类别（0~9）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corss_entropy(y_hat, y):\n",
    "    # (batch_size, q)--->(batch_size)\n",
    "    return - torch.log(y_hat[range(len(y_hat)), y])\n",
    "# return - torch.log(y_hat[:, y]) 错误原因：切片和索引混用，维度变成二维"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 分类精度  \n",
    "给定预测概率分布y_hat，当我们必须输出硬预测（hard prediction）时，我们通常选择预测概率最高的类。分类精度即正确预测数量与总预测数量之比。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_hat, y):\n",
    "    \"\"\"计算预测正确的数量\"\"\"\n",
    "    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:\n",
    "        y_hat = y_hat.argmax(axis=1)\n",
    "    cmp = y_hat.type(y.dtype) == y\n",
    "    return float(cmp.type(y.dtype).sum())\n",
    "\n",
    "def evaluuate_accuracy(net, data_iter):\n",
    "    \"\"\"计算在指定数据集上模型的精度\"\"\"\n",
    "    if isinstance(net, torch.nn.Module):\n",
    "        net.eval()\n",
    "    metric = Accumulator(2)\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_iter:\n",
    "            metric.add(accuracy(net(X), y), y.numel())\n",
    "    return metric[0] / metric[1]\n",
    "\n",
    "class Accumulator():\n",
    "    \"\"\"在n个变量上累加\"\"\"\n",
    "    def __init__(self, n):\n",
    "        self.data = [0.0] * n\n",
    "\n",
    "    def add(self, *args):\n",
    "        self.data = [a + float(b) for a, b in zip(self.data, args)]\n",
    "    \n",
    "    def reset(self):\n",
    "        self.data = [0.0] * len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 定义优化算法  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1\n",
    "def updater(batch_size):\n",
    "    return d2l.sgd([W, b], lr, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch_ch3(net, train_iter, loss, updater):\n",
    "    \"\"\"训练模型一个迭代周期\"\"\"\n",
    "    if isinstance(net, torch.nn.Module):\n",
    "        net.train()\n",
    "    \n",
    "    # 训练损失总和、训练准确度总和、样本数\n",
    "    metric = Accumulator(3)\n",
    "    for X, y in train_iter:\n",
    "        # 计算梯度并更新参数\n",
    "        y_hat = net(X)\n",
    "        l = loss(y_hat, y)\n",
    "        if isinstance(updater, torch.optim.Optimizer):\n",
    "            # 使用PyTorch内置的优化器和损失函数\n",
    "            updater.zero_grad()\n",
    "            l.mean().backward()\n",
    "            updater.step()\n",
    "        else:\n",
    "            # 使用定制的优化器和损失函数\n",
    "            l.sum().backward()\n",
    "            updater(X.shape[0])\n",
    "        metric.add(float(l.sum()), accuracy(y_hat, y), y.numel()) # l.sum().backward()销毁了计算图，l.sum()不会影响W,b的梯度\n",
    "    # 返回平均训练损失和训练精度    \n",
    "    return metric[0] / metric[2], metric[1] / metric[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "该训练函数将会运行多个迭代周期（由num_epochs指定）。在每个迭代周期结束时，利用test_iter访问到的测试数据集对模型进行评估。我们将利用Animator类来可视化训练进度。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Animator():\n",
    "    \"\"\"在动画中绘制数据\"\"\"\n",
    "    def __init__(self, xlabel=None, ylabel=None, legned=None, xlim=None, \n",
    "                 ylim=None, xscale='linear', yscale='linear', \n",
    "                 fmts=('-', 'm--', 'g-.', 'r:'), nrows=1, ncols=1, \n",
    "                 figsize=(3.5, 2.5)):\n",
    "        # 增量地绘制多条线\n",
    "        if legned is None:\n",
    "            legned = []\n",
    "        self.fig, self.axes = d2l.plt.subplots(nrows, ncols, figsize=figsize)\n",
    "        if nrows * ncols == 1:\n",
    "            self.axes = [self.axes, ]\n",
    "        # 使用lambda函数捕获参数\n",
    "        self.config_axes = lambda: d2l.set_axes(self.axes[0], xlabel, ylabel, xlim, ylim, \n",
    "                                                xscale, yscale, legned)\n",
    "        self.X, self.Y, self.fmts = None, None, fmts\n",
    "    \n",
    "    def add(self, x, y):\n",
    "        # 向图表中添加多个数据点\n",
    "        if not hasattr(y, \"__len__\"):\n",
    "            y = [y]\n",
    "        n = len(y)\n",
    "        if not hasattr(x, \"__len__\"):\n",
    "            x = [x] * n\n",
    "        if not self.X:\n",
    "            self.X = [[] for _ in range(n)]\n",
    "        if not self.Y:\n",
    "            self.Y = [[] for _ in range(n)]\n",
    "        for i, (a, b) in enumerate(zip(x, y)):\n",
    "            if a is not None and b is not None:\n",
    "                self.X[i].append(a)\n",
    "                self.Y[i].append(b)\n",
    "        self.axes[0].cla() # Clear the Axes\n",
    "        for x, y, fmt in zip(self.X, self.Y, self.fmts):\n",
    "            self.axes[0].plot(x, y, fmt)\n",
    "        self.config_axes()\n",
    "        # display.display(self.fig)\n",
    "        # display.clear_output(wait=True)\n",
    "\n",
    "def train_ch3(net, train_iter, test_iter, loss, num_epochs, updater):\n",
    "    \"\"\"训练模型\"\"\"\n",
    "    animator = Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0.3, 0.9],\n",
    "                        legned=['train loss', 'train acc', 'test acc'])\n",
    "    for epoch in range(num_epochs):\n",
    "        train_metrics = train_epoch_ch3(net, train_iter, loss, updater)\n",
    "        test_acc = evaluuate_accuracy(net, test_iter)\n",
    "        animator.add(epoch + 1, train_metrics + (test_acc,))\n",
    "    d2l.plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "train_ch3(net, train_iter, test_iter, corss_entropy, num_epochs, updater)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ch3(net, test_iter, n=6):\n",
    "    for X, y in test_iter:\n",
    "        break\n",
    "    trues = d2l.get_fashion_mnist_labels(y)\n",
    "    preds = d2l.get_fashion_mnist_labels(net(X).argmax(axis=1))\n",
    "    print(trues)\n",
    "    print(preds)\n",
    "\n",
    "predict_ch3(net, test_iter)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
