# 正则化  
正则化技术主要用于解决深度网络的**过拟合现象**，主要包含两个方法：权重衰减（weight decay）和暂退法（Dropuot）。假设我们已经拥有尽可能多的高质量数据，我们便可以将重点放在正则化技术上。  

1.权重衰减  
控制权重$\bold w$的取值范围，一般要求$\bold w$尽可能的小，进而降低模型复杂度。  
如何衡量$\bold w$的大小？ 答：将损失函数中加入$L_2$范数作为惩罚项。  
以线性回归为例，损失函数变为如下形式：  
$$
L(\bold w, b)+\frac{\lambda}{2}||\bold w||^2
$$
对于$\lambda = 0$，我们恢复了原来的损失函数。对于$\lambda > 0$，我们限制$||\bold w||$的大小。  
$L_2$正则化回归的小批量随机梯度下降更新如下式：  
$$
\bold w \leftarrow (1-\eta\lambda)\bold w - \frac{\eta}{|\mathcal B|}\sum_{i\in\mathcal B}\bold x^{(i)}(\bold w^\top \bold x^{(i)}+b-y^{(i)})
$$
$0< \eta\lambda < 1$保证了权重衰减，其中$\eta，\lambda$是超参数。  

2.暂退法  
在训练过程中，在计算后续层之前向网络的每一层注入噪声。因为当训练一个有多层的深层网络时，注入噪声只会在输入‐输出映射上增强平滑性，越平滑往往意味着不易过拟合。  
在标准暂退法正则化中，通过按保留（未丢弃）的节点的分数进行规范化来消除每一层的偏差。换言之，每个中间活性值$h$以暂退概率$p$由随机变量$h'$替换，如下所示：  
$$
h'=
\begin{cases}
0&\text{概率为}p\\
\frac{h}{1-p}&\text{其他情况}
\end{cases}
$$
根据此模型的设计，其期望值保持不变，即$E[h'] = h$。

在实际中将暂退法作为层，应用到每个隐藏层输出上。下面是一个简单实例：  
![暂退法效果](picture\dropout.jpg)  
### 注意： 通常，我们在测试时不用暂退法。  