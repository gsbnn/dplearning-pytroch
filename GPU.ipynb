{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU  \n",
    "在PyTorch中，每个数组都有一个设备（device），我们通常将其称为环境（context）。默认情况下，所有变量和相关的计算都分配给CPU。有时环境可能是GPU。  \n",
    "\n",
    "在带有GPU的服务器上训练神经网络时，我们通常希望模型的参数在GPU上。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 计算设备  \n",
    "我们可以指定用于存储和计算的设备，如CPU和GPU。默认情况下，张量是在内存中创建的，然后使用CPU计算它。\n",
    "\n",
    "在PyTorch中， CPU和GPU可以用`torch.device('cpu')` 和`torch.device('cuda')`表示。应该注意的是，cpu设备意味着所有物理CPU和内存，这意味着PyTorch的计算将尝试使用所有CPU核心。然而，gpu设备只代表一个卡和相应的显存。如果有多个GPU，我们使用`torch.device(f'cuda:{i}')` 来表示第i块GPU（i从0开始）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "cuda:0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "\n",
    "print(torch.device('cpu'))\n",
    "print(torch.device('cuda:0'))\n",
    "print(torch.cuda.device_count()) # 查询可用gpu的数量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 张量与GPU  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 我们可以查询张量所在的设备。默认情况下，张量是在CPU上创建的。\n",
    "x = torch.tensor([1, 2, 3])\n",
    "x.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.]], device='cuda:0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 存储在GPU上\n",
    "X = torch.ones(2, 3, device=torch.device('cuda:0'))\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不在同一GPU上的tensor如何进行计算？ \n",
    " \n",
    "![GPU复制](picture\\GPU_copy.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Z = X.cuda(1)\n",
    "print(X)\n",
    "print(Z)\n",
    "\n",
    "output:\n",
    "tensor([[1., 1., 1.],\n",
    "        [1., 1., 1.]], device='cuda:0')\n",
    "tensor([[1., 1., 1.],\n",
    "        [1., 1., 1.]], device='cuda:1')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 神经网络与GPU  \n",
    "将模型参数放到GPU上。  \n",
    "当输入为GPU上的张量时，模型将在同一GPU上计算结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0319],\n",
      "        [0.0319]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "net = nn.Sequential(nn.Linear(3, 1))\n",
    "net = net.to(device=torch.device('cuda:0'))\n",
    "\n",
    "print(net(X))\n",
    "print(net[0].weight.data.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU在大数据情况下优势明显。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time2: 6.851683616638184\n"
     ]
    }
   ],
   "source": [
    "X_cpu = torch.randn(30000, 20000, dtype=torch.float32)\n",
    "Y_cpu = torch.randn(20000, 2000, dtype=torch.float32)\n",
    "X_gpu = torch.randn(30000, 20000, dtype=torch.float32, device=torch.device('cuda:0'))\n",
    "Y_gpu = torch.randn(20000, 2000, dtype=torch.float32, device=torch.device('cuda:0'))\n",
    "timer = d2l.Timer()\n",
    "torch.matmul(X_cpu, Y_cpu)\n",
    "time1 = timer.stop()\n",
    "timer.start()\n",
    "torch.matmul(X_gpu, Y_gpu)\n",
    "time2 = timer.stop()\n",
    "print(f'time1:{time1}, time2:{time2}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
