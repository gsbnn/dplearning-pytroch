# 从全连接层到卷积  
假设我们有一个足够充分的照片数据集，数据集中是拥有标注的照片，每张照片具有百万级像素，这意味着网络的每次输入都有一百万个维度。即使将隐藏层维度降低到$1000$，这个全连接层也将有$10^6\times10^3=10^9$个参数。想要训练这个模型将不可实现。 

然而，如今人类和机器都能很好地区分猫和狗：这是因为图像中本就拥有丰富的结构，而这些结构可以被人类和机器学习模型使用。卷积神经网络（convolutional neural networks， CNN）是机器学习利用自然图像中一些已知结构的创造性方法。  

我们将以用于计算机视觉的**2D卷积**为例，介绍深度学习中的卷积。想象一下，假设我们想从一张图片中找到某个物体，应满足以下两个原则：  
> - 平移不变性（translation invariance）：不管检测对象出现在图像中的哪个位置，神经网络的前面几层应该对相同的图像区域具有相似的反应，即为“平移不变性”。  
> - 局部性（locality）：神经网络的前面几层应该只探索输入图像中的局部区域，而不过度在意图像中相隔较远区域的关系，这就是“局部性”原则。最终，可以聚合这些局部特征，以在整个图像级别进行预测。  

1.多层感知机的限制  
首先，多层感知机的输入是二维图像$\bold X$，其隐藏表示$\bold H$在数学上是一个矩阵，在代码中表示为二维张量。其中$\bold X$和$\bold H$具有相同的形状。使用$[\bold X]_{i,j}$和$[\bold H]_{i,j}$分别表示输入图像和隐藏表示中位置$(i,j)$处的像素，$\bold U$为偏置参数，权重参数$\mathsf W$是一个四维张量。  

a.假设不满足平移不变性，我们可以将全连接层形式化地表示为：  
$$
\begin{align}
[\bold H]_{i,j}&=[\bold U]_{i,j}+\sum_k\sum_l[\mathsf W]_{i,j,k,l}[\bold X]_{k,l}\\
&=[\bold U]_{i,j}+\sum_a\sum_b[\mathsf V]_{i,j,a,b}[\bold X]_{i+a,j+b}\\
\end{align}
$$
其中$[\mathsf V]_{i,j,a,b}=[\mathsf W]_{i,j,i+a,j+b}$。上式说明：对于隐藏表示中任意给定位置$(i,j)$处的像素值$[\bold H]_{i,j}$，可以通过以$[\bold X]_{i,j}$为中心对一定范围内（由$a,b$决定）的像素进行加权求和得到，加权使用的权重为$[\mathsf V]_{i,j,a,b}$。
**总结：对于$(i,j)$处的像素值$[\bold X]_{i,j}$，需要用$(i,j)$处的权重$[\mathsf V]_{i,j,a,b}$相乘求和**  

b.假设满足平移不变性，也就是说，$\mathsf V$和$\bold U$实际上不依赖于$(i, j)$的值，因此，我们可以简化$\bold H$定义为：  
$$
[\bold H]_{i,j}=u+\sum_a\sum_b[\bold V]_{a,b}[\bold X]_{i+a,j+b}
$$  
其中$[\bold V]_{a,b}=[\bold W]_{i+a,j+b}$。这就是卷积（convolution）。我们是在使用系数$[\bold V]_{a,b}$对位置$(i, j)$附近的像素$(i + a, j + b)$进行加权得到$[\bold H]_{i,j}$。注意，$[\bold V]_{a,b}$的系数比$[\mathsf V]_{i,j,a,b}$少很多，因为前者不再依赖于图像中的位置。这就是显著的进步！  

c.假设进一步考虑局部性，我们需要对$a,b$作出一定的限制，即当$|a|>\Delta$或$|b|>\Delta$时，设置$[\bold V]_{a,b}=0$。因此，我们可以将$\bold H$定义为：  
$$
[\bold H]_{i,j}=u+\sum_{a=-\Delta}^\Delta\sum_{b=-\Delta}^\Delta[\bold V]_{a,b}[\bold X]_{i+a,j+b}
$$

d.考虑图像往往有多个通道，比如三原色，因此将：
$$
\begin{align}
[\bold X]_{i,j}&\rightarrow[\mathsf X]_{i,j,c}\\
[\bold V]_{a,b}&\rightarrow[\mathsf V]_{a,b,c}\\
u&\rightarrow[\bold u]_{c}
\end{align}
$$
其中$c$为输入通道数。同时我们希望输出也是多通道的，因此将：  
$$
\begin{align}
[\bold H]_{i,j}&\rightarrow[\mathsf H]_{i,j,d}\\
[\mathsf V]_{a,b,c}&\rightarrow[\mathsf V]_{a,b,c,d}\\
[\bold u]_{c}&\rightarrow[\bold U]_{c,d}
\end{align}
$$
其中$d$为输出通道数，最终得到多通道输入多通道输出的卷积操作:  
$$
[\mathsf H]_{i,j,d}=\sum_{a=-\Delta}^\Delta\sum_{b=-\Delta}^\Delta\sum_c[\bold U]_{c,d}+[\mathsf V]_{a,b,c,d}[\mathsf X]_{i+a,j+b,c}
$$

2.卷积的实际应用——图像卷积  
上节我们解析了卷积层的原理，现在我们看看它的实际应用。由于卷积神经网络的设计是用于探索图像数据，本节我们将以图像为例。  
a.互相关运算  
严格来说，卷积层是个错误的叫法，因为它所表达的运算其实是互相关运算（cross‐correlation），而不是卷积运算。  
首先，我们暂时忽略通道（第三维）这一情况，看看如何处理二维图像数据和隐藏表示。首先，我们暂时忽略通道（第三维）这一情况，看看如何处理二维图像数据和隐藏表示。在 图6.2.1中，输入是高度为3、宽度为3的二维张量（即形状为3 × 3）。卷积核的高度和宽度都是2，而卷积核窗口（或卷积窗口）的形状由内核的高度和宽度决定（即2 × 2）。  
![二维互相关运算](picture\cross‐correlation.jpg)  
在二维互相关运算中，卷积窗口从输入张量的左上角开始，从左到右、从上到下滑动。当卷积窗口滑动到新一个位置时，包含在该窗口中的部分张量与卷积核张量进行**按元素相乘**，得到的张量再求和得到一个**单一的标量值**，由此我们得出了这一位置的输出张量值。在如上例子中，输出张量的四个元素由二维互相关运算得到，这个输出高度为2、宽度为2，如下所示：  
$$
0\times 0+1\times 1+3\times 2+4\times 3=19,\\
1\times 0+2\times 1+4\times 2+5\times 3=25,\\
3\times 0+4\times 1+6\times 2+7\times 3=37,\\
4\times 0+5\times 1+7\times 2+8\times 3=43.
$$  
注意，输出大小略小于输入大小。这是因为卷积核的宽度和高度大于1，而卷积核只与图像中每个大小完全适合的位置进行互相关运算。所以，输出大小等于输入大小$n_h\times n_w$减去卷积核大小$k_h\times k_w$，即：  
$$
(n_h-k_h+1)\times (n_w-k_w+1)
$$